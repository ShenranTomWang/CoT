{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.loading_utils import get_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOP_K = 100        # Top k activations to look at\n",
    "DATA_DIR = \"./experimental_data/\"\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "print(f\"Executing on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "diffs_resid = torch.load(DATA_DIR + \"diffs_resid.pt\")\n",
    "acts_resid = torch.load(DATA_DIR + \"acts_resid.pt\")\n",
    "acts_exp_resid = torch.load(DATA_DIR + \"acts_exp_resid.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mean_diffs_resid = torch.mean(diffs_resid, dim=0)\n",
    "std_diffs_resid = torch.std(diffs_resid, dim=0)\n",
    "mean_exp_resid = torch.mean(acts_exp_resid, dim=0)\n",
    "std_exp_resid = torch.std(acts_exp_resid, dim=0)\n",
    "mean_resid = torch.mean(acts_resid, dim=0)\n",
    "std_resid = torch.std(acts_resid, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def print_mean_std(array: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prints the mean and standard deviation of the array.\n",
    "    args:\n",
    "        array: The array to print the mean and standard deviation of.\n",
    "    \"\"\"\n",
    "    for i in range(array.shape[0]):\n",
    "        print(f\"Layer {i}\")\n",
    "        # print(f\"mean: {mean_diffs_resid[i]}\")\n",
    "        # print(f\"std: {std_diffs_resid[i]}\")\n",
    "        print(f\"mean std: {torch.mean(array[i])}\")\n",
    "\n",
    "print_mean_std(std_exp_resid)\n",
    "print_mean_std(std_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find CoT related activations by averaging out irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_min_std_indeces(std_resid: torch.Tensor, k: int):\n",
    "    \"\"\"\n",
    "    Gets the indices of the minimum standard deviation for each layer.\n",
    "    args:\n",
    "        std_resid: The standard deviation of the activations.\n",
    "        k: Top k minimum standard deviations to return.\n",
    "    returns:\n",
    "        The indices of the minimum standard deviation for each layer.\n",
    "    \"\"\"\n",
    "    indeces = []\n",
    "    for i in range(std_resid.shape[0]):\n",
    "        indeces_layer_i = torch.argsort(std_resid[i, :], dim=0, descending=False)[:k]\n",
    "        indeces.append(indeces_layer_i)\n",
    "    indeces = torch.stack(indeces)\n",
    "    return indeces\n",
    "\n",
    "cot_diffs_indeces = get_min_std_indeces(std_diffs_resid, TOP_K)\n",
    "cot_exp_indeces = get_min_std_indeces(std_exp_resid, TOP_K)\n",
    "cot_indeces = get_min_std_indeces(std_resid, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(tensor: torch.Tensor, title: str, axis=None, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Draws a heatmap of the tensor.\n",
    "    args:\n",
    "        tensor: The tensor to draw the heatmap of. Must have dimensionality of 2\n",
    "        title: The title of the heatmap.\n",
    "        axis: The axis to draw the heatmap on (for side-by-side plots), default to None if not needed\n",
    "        vmin: The minimum value to draw on heatmap, defaults to None\n",
    "        vmax: The maximum value to draw on heatmap, defaults to None\n",
    "    \"\"\"\n",
    "    data = tensor.to(torch.float32).cpu().numpy()\n",
    "    sns.heatmap(data, cmap='viridis', cbar=True, xticklabels=False, yticklabels=True, ax=axis, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax = axis if axis is not None else plt.gca()\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('Layer')\n",
    "    ax.set_title(title)\n",
    "\n",
    "layer_avg_std_exp_resid = torch.mean(std_exp_resid, dim=1).unsqueeze(1)\n",
    "layer_avg_std_resid = torch.mean(std_resid, dim=1).unsqueeze(1)\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "vmin = torch.min(torch.min(std_exp_resid), torch.min(std_resid))\n",
    "vmax = torch.max(torch.max(std_exp_resid), torch.max(std_resid))\n",
    "draw_heatmap(std_exp_resid, \"std_exp_resid\", axes[0], vmin=vmin, vmax=vmax)\n",
    "draw_heatmap(std_resid, \"std_resid\", axes[1], vmin=vmin, vmax=vmax)\n",
    "vmin = torch.min(torch.min(layer_avg_std_exp_resid), torch.min(layer_avg_std_resid))\n",
    "vmax = torch.max(torch.max(layer_avg_std_exp_resid), torch.max(layer_avg_std_resid))\n",
    "draw_heatmap(layer_avg_std_exp_resid, \"layer_avg_std_exp_resid\", axes[2], vmin=vmin, vmax=vmax)\n",
    "draw_heatmap(layer_avg_std_resid, \"layer_avg_std_resid\", axes[3], vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the middle layers have higher std for non-CoT. We can also notice a feature with very high std at the last layer for CoT. Looking at the mean of std at each layer, it seems like our claim can be verified. It is possible that non-CoT started retrieving knowledge in earlier layers, maybe CoT neuron lies in the earlier layers such that non-CoT is not signaled with CoT and therefore started retrieving knowledge earlier. This is consistent with our findings on llama-3.2-1b:\n",
    "![](./figs/llama-3.2-1b-std_resid_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "layer_avg_mean_exp_resid = torch.mean(mean_exp_resid, dim=1).unsqueeze(1)\n",
    "layer_avg_mean_resid = torch.mean(mean_resid, dim=1).unsqueeze(1)\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "vmin = torch.min(torch.min(mean_exp_resid), torch.min(mean_resid))\n",
    "vmax = torch.max(torch.max(mean_exp_resid), torch.max(mean_resid))\n",
    "draw_heatmap(mean_exp_resid, \"mean_exp_resid\", axes[0], vmin=vmin, vmax=vmax)\n",
    "draw_heatmap(mean_resid, \"mean_resid\", axes[1], vmin=vmin, vmax=vmax)\n",
    "vmin = torch.min(torch.min(layer_avg_mean_exp_resid), torch.min(layer_avg_mean_resid))\n",
    "vmax = torch.max(torch.max(layer_avg_mean_exp_resid), torch.max(layer_avg_mean_resid))\n",
    "draw_heatmap(layer_avg_mean_exp_resid, \"layer_avg_mean_exp_resid\", axes[2], vmin=vmin, vmax=vmax)\n",
    "draw_heatmap(layer_avg_mean_resid, \"layer_avg_mean_resid\", axes[3], vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means of Non-CoT gets smaller per layer, while Non-CoT jumps back-and-forth across different layers. It is worth noticing that Non-CoT gets much smaller compared to CoT down the layers. This could be due to retierval of knowledge in early layers that guided misinformation which propagated down the pipeline, whereas CoT does not attempt to retrieve all the information at once. Activations from gemma-2-2b-it is slightly different from our observations on llama-3.2-1b. However, gemma-2-2b-it also have a concentrated high-mean area around its mid to later layers Below are results from llama-3.2-1b\n",
    "![](./figs/llama-3.2-1b-mean_resid_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "layer_mean_diffs_resid = torch.mean(mean_diffs_resid, dim=1).unsqueeze(1)\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "draw_heatmap(mean_diffs_resid, \"mean_diffs_resid\", axes[0])\n",
    "draw_heatmap(layer_mean_diffs_resid, \"layer_mean_diffs_resid\", axes[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
